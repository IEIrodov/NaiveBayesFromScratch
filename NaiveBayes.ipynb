{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"NaiveBayes.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOdS8Rr9+GLnXupzGsZnuJk"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"IqoVzKPVtBTi","executionInfo":{"status":"ok","timestamp":1630478422419,"user_tz":-330,"elapsed":375,"user":{"displayName":"Nikhil Talks","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjEpz4XvO9RD_oqQJzxZI7k02vbOMJSW8W7FYDw=s64","userId":"11310803230865859121"}}},"source":["import numpy as np\n","from sklearn import datasets"],"execution_count":27,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"A-LnIkr2rKrz"},"source":["# Fit Function"]},{"cell_type":"code","metadata":{"id":"nd2k6qzmmKqh","executionInfo":{"status":"ok","timestamp":1630479318127,"user_tz":-330,"elapsed":406,"user":{"displayName":"Nikhil Talks","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjEpz4XvO9RD_oqQJzxZI7k02vbOMJSW8W7FYDw=s64","userId":"11310803230865859121"}}},"source":["def fit(X_train,Y_train):\n","  result = {}                   #dict correspond to every possible value\n","  class_values = set(Y_train)   #to find distinct values in y\n","  for current_class in class_values:\n","    result[current_class ] = {}  #dict of all possible features\n","    result[\"total_data\"] = len(Y_train)\n","    current_class_rows = (Y_train == current_class) #training data which has the class as current class\n","    X_train_current = X_train[current_class_rows] #we'll get those rows where value is true bcz in above step we got an array of true false values\n","    Y_train_current = Y_train[current_class_rows]\n","    num_features = X_train.shape[1] #m*n and number of features is n i.e. columns\n","    result[current_class][\"total_count\"] = len(Y_train_current)#total count of training data which belongs to the current class\n","    for j in range(1,num_features+1):\n","      j_1 = j-1 #to keep the feature names from 1 to n \n","      result[current_class][j] = {}   #all the possible value a particular feature can take\n","      all_possible_values = set(X_train[:,j_1])  #[:,j_1] only the Jth column and set will get us the unique values in it\n","      for current_value in all_possible_values:\n","        result[current_class][j][current_value] =  (X_train_current[:,j_1] == current_value).sum() #for each value we need to store the count, count of all training datapoints where y is current class how many of them jth value is the current value\n","  return result"],"execution_count":49,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"g2oQJG75rGRm"},"source":["# Predict Function"]},{"cell_type":"code","metadata":{"id":"jfSJwULv49Qz","executionInfo":{"status":"ok","timestamp":1630479319624,"user_tz":-330,"elapsed":7,"user":{"displayName":"Nikhil Talks","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjEpz4XvO9RD_oqQJzxZI7k02vbOMJSW8W7FYDw=s64","userId":"11310803230865859121"}}},"source":["def probability(dictionary,x,current_class):\n","  output = np.log(dictionary[current_class][\"total_count\"]/dictionary[\"total_data\"])  #using log probab instead of prob\n","  num_features = len(dictionary[current_class].keys())-1; #number of features we have\n","  for j in range(1,num_features+1):\n","    xj = x[j-1]\n","    count_current_class_with_value_xj = dictionary[current_class][j][xj]+1 #Laplace Correction\n","    count_current_class = dictionary[current_class][\"total_count\"] + len(dictionary[current_class][j].keys()) #how many different values j can have\n","    current_xj_probablity = np.log(count_current_class_with_value_xj) - np.log(count_current_class)\n","    output = output + current_xj_probablity\n","  return output\n","\n"],"execution_count":50,"outputs":[]},{"cell_type":"code","metadata":{"id":"2Im_Uc1gr8Bd","executionInfo":{"status":"ok","timestamp":1630479319626,"user_tz":-330,"elapsed":8,"user":{"displayName":"Nikhil Talks","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjEpz4XvO9RD_oqQJzxZI7k02vbOMJSW8W7FYDw=s64","userId":"11310803230865859121"}}},"source":["def predictSinglePoint(dictionary,x):\n","  classes = dictionary.keys()   #.keys() returns all the variable keys in the dictionary\n","  best_p = -1000\n","  best_class = -1\n","  first_run = True\n","  for current_class in classes:\n","    if current_class == \"total_data\":\n","      continue\n","    p_current_class = probability(dictionary,x,current_class)\n","    if(first_run or p_current_class>best_p):\n","      best_p = p_current_class\n","      best_class = current_class\n","    first_run = False\n","  return best_class"],"execution_count":51,"outputs":[]},{"cell_type":"code","metadata":{"id":"ABkZwZixqPnt","executionInfo":{"status":"ok","timestamp":1630479320219,"user_tz":-330,"elapsed":2,"user":{"displayName":"Nikhil Talks","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjEpz4XvO9RD_oqQJzxZI7k02vbOMJSW8W7FYDw=s64","userId":"11310803230865859121"}}},"source":["def predict(dictionary,X_test):\n","  y_pred = []\n","  for x in X_test:\n","    x_class = predictSinglePoint(dictionary,x)\n","    y_pred.append(x_class)\n","  return y_pred "],"execution_count":52,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"e_zxKQDzFrZd"},"source":["# Iris DataSet"]},{"cell_type":"markdown","metadata":{"id":"ldGT13huF3ij"},"source":["#### Converting continous data to Discrete data for classification"]},{"cell_type":"code","metadata":{"id":"8aSTK2ebryIg","executionInfo":{"status":"ok","timestamp":1630479320620,"user_tz":-330,"elapsed":7,"user":{"displayName":"Nikhil Talks","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjEpz4XvO9RD_oqQJzxZI7k02vbOMJSW8W7FYDw=s64","userId":"11310803230865859121"}}},"source":["def makeLabelled(column):\n","  second_limit = column.mean()\n","  first_limit = 0.5*second_limit\n","  third_limit = 1.5*second_limit\n","  for i in range(0,len(column)):\n","    if(column[i]<first_limit):\n","      column[i] = 0\n","    elif (column[i]<second_limit):\n","      column[i] = 1\n","    elif column[i]<third_limit:\n","      column[i] = 2\n","    else:\n","      column[i] = 3\n","  return column"],"execution_count":53,"outputs":[]},{"cell_type":"code","metadata":{"id":"x3SIxQNIFzUj","executionInfo":{"status":"ok","timestamp":1630479320623,"user_tz":-330,"elapsed":9,"user":{"displayName":"Nikhil Talks","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjEpz4XvO9RD_oqQJzxZI7k02vbOMJSW8W7FYDw=s64","userId":"11310803230865859121"}}},"source":["iris = datasets.load_iris()\n","X = iris.data\n","Y = iris.target"],"execution_count":54,"outputs":[]},{"cell_type":"code","metadata":{"id":"sPQy3KAkGxqf","executionInfo":{"status":"ok","timestamp":1630479320624,"user_tz":-330,"elapsed":8,"user":{"displayName":"Nikhil Talks","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjEpz4XvO9RD_oqQJzxZI7k02vbOMJSW8W7FYDw=s64","userId":"11310803230865859121"}}},"source":["for i in range(0,X.shape[-1]):\n","  X[:,i] = makeLabelled(X[:,i])"],"execution_count":55,"outputs":[]},{"cell_type":"code","metadata":{"id":"JjO5833eH-ge","executionInfo":{"status":"ok","timestamp":1630479321056,"user_tz":-330,"elapsed":5,"user":{"displayName":"Nikhil Talks","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjEpz4XvO9RD_oqQJzxZI7k02vbOMJSW8W7FYDw=s64","userId":"11310803230865859121"}}},"source":["from sklearn.model_selection import train_test_split\n","X_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size=0.25,random_state=0) #split of 75 25"],"execution_count":56,"outputs":[]},{"cell_type":"code","metadata":{"id":"Y4yzIAluISEn","executionInfo":{"status":"ok","timestamp":1630479321058,"user_tz":-330,"elapsed":6,"user":{"displayName":"Nikhil Talks","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjEpz4XvO9RD_oqQJzxZI7k02vbOMJSW8W7FYDw=s64","userId":"11310803230865859121"}}},"source":["dictionary = fit(X_train,Y_train)"],"execution_count":57,"outputs":[]},{"cell_type":"code","metadata":{"id":"jr9zHv_jIft9","executionInfo":{"status":"ok","timestamp":1630479350991,"user_tz":-330,"elapsed":393,"user":{"displayName":"Nikhil Talks","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjEpz4XvO9RD_oqQJzxZI7k02vbOMJSW8W7FYDw=s64","userId":"11310803230865859121"}}},"source":["Y_pred = predict(dictionary,X_test)"],"execution_count":58,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AJ7LYi9tJHjP","executionInfo":{"status":"ok","timestamp":1630484882258,"user_tz":-330,"elapsed":379,"user":{"displayName":"Nikhil Talks","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjEpz4XvO9RD_oqQJzxZI7k02vbOMJSW8W7FYDw=s64","userId":"11310803230865859121"}},"outputId":"84b82061-685a-49c5-a126-4491ca8778f8"},"source":["from sklearn.metrics import classification_report,confusion_matrix\n","print(classification_report(Y_test,Y_pred))\n","print(confusion_matrix(Y_test,Y_pred))"],"execution_count":60,"outputs":[{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       1.00      1.00      1.00        13\n","           1       0.94      1.00      0.97        16\n","           2       1.00      0.89      0.94         9\n","\n","    accuracy                           0.97        38\n","   macro avg       0.98      0.96      0.97        38\n","weighted avg       0.98      0.97      0.97        38\n","\n","[[13  0  0]\n"," [ 0 16  0]\n"," [ 0  1  8]]\n"]}]},{"cell_type":"markdown","metadata":{"id":"KynG46hoePAa"},"source":["### Naive Bayes using Continous Value without converting them Discrete"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tEn-M00JJV4C","executionInfo":{"status":"ok","timestamp":1630485009758,"user_tz":-330,"elapsed":405,"user":{"displayName":"Nikhil Talks","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjEpz4XvO9RD_oqQJzxZI7k02vbOMJSW8W7FYDw=s64","userId":"11310803230865859121"}},"outputId":"f3fc09c9-7d30-4f23-c8a3-e26573584248"},"source":["from sklearn.naive_bayes import GaussianNB\n","clf = GaussianNB()\n","clf.fit(X_train,Y_train)\n","Y_pred = clf.predict(X_test)\n","print(classification_report(Y_test,Y_pred))\n","print(confusion_matrix(Y_test,Y_pred))"],"execution_count":63,"outputs":[{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       1.00      0.85      0.92        13\n","           1       0.76      1.00      0.86        16\n","           2       1.00      0.67      0.80         9\n","\n","    accuracy                           0.87        38\n","   macro avg       0.92      0.84      0.86        38\n","weighted avg       0.90      0.87      0.87        38\n","\n","[[11  2  0]\n"," [ 0 16  0]\n"," [ 0  3  6]]\n"]}]},{"cell_type":"markdown","metadata":{"id":"khvj9y1qe3A5"},"source":["## Conclusion:- Our accuracy is decreased in this case because Iris dataset points are not strictly following the Gaussian Curve but for most of the datasets having continous value our GaussianNB() works great and accuracy improved majorly"]},{"cell_type":"code","metadata":{"id":"LxgJzWyneYIu"},"source":[""],"execution_count":null,"outputs":[]}]}